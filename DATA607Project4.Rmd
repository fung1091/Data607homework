---
title: "DATA607Project4"
author: "jim lung"
date: "04-10-2017"
output: html_document
---

http://spamassassin.apache.org/old/publiccorpus/

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:  https://spamassassin.apache.org/publiccorpus/

Here are two short videos that you may find helpful.

The first video shows how to unzip the provided files.

```{r }

if (!require(stringr)) install.packages('stringr')
if (!require(tm)) install.packages('tm')
if (!require(SnowballC)) install.packages('SnowballC')
if (!require(RTextTools)) install.packages('RTextTools')
if (!require(R.utils)) install.packages('R.utils')
if (!require(utils)) install.packages('utils')

#tmp <- readLines("607ass10/easy_ham/00001.7c53336b37003a9286aba55d2945844c")
#tmp <- str_c(tmp, collapse = "")
#tmp <- str_replace_all(tmp, pattern="<.*?>", replacement = " ")
#tmp <- str_replace_all(tmp, pattern="\\=", replacement = "")

```


```{r}
require(R.utils)

download.file('http://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2', destfile="spam_zip.tar.bz2")
bunzip2("spam_zip.tar.bz2", remove = F, overwrite = T)
untar("spam_zip.tar") #creates spam_2 folder

# download and unzip spam document from github 
download.file('http://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2', destfile="ham_zip.tar.bz2")
bunzip2("ham_zip.tar.bz2", remove = F, overwrite = T)
untar("ham_zip.tar") #creates easy_ham_2 folder

```

```{r}
# identify extraneous ham file and delete
removeham <- list.files(path="easy_ham_2/", full.names=T, recursive=FALSE, pattern="cmds")
file.remove(removeham)

# identify extraneous spam file and delete
removespam <-list.files(path="spam_2/", full.names=T, recursive=FALSE, pattern="cmds")
file.remove(removespam)

```

```{r}
# list of spam files
spamfiles <-list.files(path="spam_2/", full.names=T, recursive=FALSE)

# list of ham files
hamfiles <- list.files(path="easy_ham_2/",full.names=T, recursive=FALSE)

# concatenate ham and spam file lists
hamspam <- c(hamfiles,spamfiles)

length(hamspam)
head(hamspam)

```

```{r}
# Vcorpus input directory 2501 files
spamfiles<-VCorpus(DirSource(directory = "spam_2/",encoding = "UTF-8"))
hamfiles<-VCorpus(DirSource(directory = "easy_ham_2/",encoding = "UTF-8"))
hamspam <- c(hamfiles,spamfiles)
```

```{r}
meta(hamspam[[2]])

```

```{r}
# remove white space
hamspam <- tm_map(hamspam,stripWhitespace)

# remove numbers
hamspam <- tm_map(hamspam,removeNumbers)

# remove punctuations
hamspam <- tm_map(hamspam,removePunctuation)
hamspam[[1]][1]

```


```{r}
# transform all words in corpus to lower case
hamspam_mod <- tm_map(hamspam, content_transformer(tolower))

# remove all stop words 
hamspam_mod <- tm_map(hamspam_mod,removeWords, words = stopwords("en"))

# stem words: cut certain terms down to word root
hamspam_mod <- tm_map(hamspam_mod, stemDocument)

```

```{r}
dtm <- DocumentTermMatrix(hamspam_mod)
dtm


```

# remove any terms less than 10 documents
```{r}
dtm <- removeSparseTerms(dtm,1-(10/length(hamspam_mod)))
dtm

```


```{r}
length(hamspam_mod)
```
`

Classifier Models

```{r}
require(RTextTools)
# create spam label vector for each email which indiciates actual status of "spam" or "not spam" 



```




# number of emails in corpus
N <- length(hamspam_mod)

# set up model container; 80/20 split between train and test data
container <- create_container(
    dtm,
    labels = spam_labels,
    trainSize = 1:(0.8*N),
    testSize = (0.8*N+1):N,
    virgin = FALSE
)

svm_model <- train_model(container, "SVM")
tree_model <- train_model(container, "TREE")
maxent_model <- train_model(container, "MAXENT")

svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)






